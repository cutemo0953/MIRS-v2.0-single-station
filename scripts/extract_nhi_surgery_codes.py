#!/usr/bin/env python3
"""
NHI å¥ä¿æ‰‹è¡“ç¢¼èƒå–å™¨ (ç¬¬ä¸ƒç¯€)
===========================

å¾å¥ä¿ç½² form4-001.pdf èƒå–ç¬¬ä¸ƒç¯€æ‰‹è¡“ç¢¼ + é»æ•¸

Usage:
    python3 scripts/extract_nhi_surgery_codes.py --pdf /path/to/form4-001.pdf --out ./data/packs/nhi_sec7

Output:
    - nhi_surgery_codes_sec7.csv
    - nhi_surgery_codes_sec7.json

Requirements:
    pip install PyMuPDF

Based on ChatGPT's dev spec for Section 7 extraction.
"""

import argparse
import csv
import json
import re
import hashlib
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, asdict

try:
    import fitz  # PyMuPDF
except ImportError:
    print("è«‹å…ˆå®‰è£ PyMuPDF: pip install PyMuPDF")
    exit(1)


# =============================================================================
# Constants
# =============================================================================

SEC7_START_MARKER = "ç¬¬äºŒéƒ¨ç¬¬äºŒç« ç¬¬ä¸ƒç¯€"
SEC8_START_MARKER = "ç¬¬äºŒéƒ¨ç¬¬äºŒç« ç¬¬å…«ç¯€"

# è¡“å¼ç¢¼æ ¼å¼: 5 ä½æ•¸å­— + 1 å¤§å¯«è‹±æ–‡
CODE_PATTERN = re.compile(r'^(\d{5}[A-Z])\s+')
CODE_ONLY = re.compile(r'^\d{5}[A-Z]$')

# é»æ•¸æ ¼å¼: 2-6 ä½ç´”æ•¸å­—
POINTS_PATTERN = re.compile(r'^(\d{2,6})$')

# æ’é™¤æ¨¡å¼: code å¾Œæ¥é “è™Ÿï¼ˆé€šå¸¸æ˜¯æ•˜è¿°æ–‡å­—ï¼‰
EXCLUDE_PATTERN = re.compile(r'\d{5}[A-Z][ã€ï¼Œ]')

# ç« ç¯€æ¨™é¡Œæ¨¡å¼
GROUP_PATTERN = re.compile(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[é …æ¬¾ç›®]')


@dataclass
class SurgeryCode:
    code: str
    points: int
    name: str
    group: str = ""
    subgroup: str = ""
    page: int = 0


# =============================================================================
# Section 7 Boundary Detection
# =============================================================================

def find_section7_pages(doc: fitz.Document) -> tuple[int, int]:
    """
    æ‰¾å‡ºç¬¬ä¸ƒç¯€çš„é é¢ç¯„åœ

    Returns:
        (start_page, end_page) - 0-indexed
    """
    sec7_pages = []
    sec8_pages = []

    for page_num in range(len(doc)):
        text = doc[page_num].get_text()
        if SEC7_START_MARKER in text:
            sec7_pages.append(page_num)
        if SEC8_START_MARKER in text:
            sec8_pages.append(page_num)

    # å¿½ç•¥ç›®éŒ„é  (é€šå¸¸åœ¨å‰ 10 é )
    # å–ç¬¬ä¸ƒç¯€æ¨™è¨˜ä¸­ï¼Œé ç¢¼æœ€å¤§çš„é€£çºŒå€æ®µ
    sec7_start = max(p for p in sec7_pages if p > 10) if any(p > 10 for p in sec7_pages) else min(sec7_pages)

    # ç¬¬å…«ç¯€é–‹å§‹é  (å¤§æ–¼ sec7_start çš„æœ€å°å€¼)
    sec8_candidates = [p for p in sec8_pages if p > sec7_start]
    sec7_end = min(sec8_candidates) - 1 if sec8_candidates else len(doc) - 1

    return sec7_start, sec7_end


# =============================================================================
# Row Extraction State Machine
# =============================================================================

def extract_codes_from_pages(doc: fitz.Document, start_page: int, end_page: int) -> List[SurgeryCode]:
    """
    å¾æŒ‡å®šé é¢ç¯„åœèƒå–è¡“å¼ç¢¼

    ä½¿ç”¨ç‹€æ…‹æ©Ÿè™•ç† PDF è·¨è¡Œ/æ¬„ä½éŒ¯ä½å•é¡Œ
    """
    results: List[SurgeryCode] = []

    current_group = ""
    current_subgroup = ""

    for page_num in range(start_page, end_page + 1):
        page = doc[page_num]
        text = page.get_text()
        lines = text.split('\n')

        pending_row: Optional[Dict[str, Any]] = None
        last_points_candidate: Optional[int] = None
        lines_since_pending = 0
        lines_since_points = 0
        prev_line = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # æª¢æŸ¥ç« ç¯€æ¨™é¡Œ
            if GROUP_PATTERN.match(line):
                if "é …" in line:
                    current_group = line
                    current_subgroup = ""
                elif "æ¬¾" in line or "ç›®" in line:
                    current_subgroup = line

            # æ’é™¤æ•˜è¿°æ®µè½ä¸­çš„ code
            if EXCLUDE_PATTERN.search(line):
                prev_line = line
                continue

            # å˜—è©¦åŒ¹é… code è¡Œ
            code_match = CODE_PATTERN.match(line)
            if code_match:
                code = code_match.group(1)
                # å– code å¾Œé¢çš„æ–‡å­—ä½œç‚ºåç¨±
                name = line[code_match.end():].strip()
                # æ¸…ç†åç¨±ä¸­çš„é»æ•¸ (å¦‚æœæ··åœ¨ä¸€èµ·)
                name = re.sub(r'\s+\d{2,6}$', '', name).strip()

                # æª¢æŸ¥æ˜¯å¦æœ‰ lastPointsCandidate å¯ç”¨ (é»æ•¸åœ¨ code ä¹‹å‰çš„æƒ…æ³)
                if last_points_candidate and lines_since_points <= 4:
                    results.append(SurgeryCode(
                        code=code,
                        points=last_points_candidate,
                        name=name,
                        group=current_group,
                        subgroup=current_subgroup,
                        page=page_num + 1
                    ))
                    last_points_candidate = None
                else:
                    # å„²å­˜ç‚º pendingï¼Œç­‰å¾…é»æ•¸
                    pending_row = {
                        'code': code,
                        'name': name,
                        'group': current_group,
                        'subgroup': current_subgroup,
                        'page': page_num + 1
                    }
                    lines_since_pending = 0

                prev_line = line
                continue

            # å˜—è©¦åŒ¹é…é»æ•¸è¡Œ
            points_match = POINTS_PATTERN.match(line)
            if points_match:
                points = int(points_match.group(1))

                # æª¢æŸ¥æ˜¯å¦ç¬¦åˆç¶å®šæ¢ä»¶
                if pending_row and lines_since_pending <= 8:
                    # æª¢æŸ¥å‰ä¸€è¡Œæ˜¯å¦æœ‰ 'v' (PDF è¡¨æ ¼åˆ†éš”ç¬¦)
                    # æˆ–è€…ç›´æ¥æ¥åœ¨ code è¡Œå¾Œé¢
                    if 'v' in prev_line.lower() or 'ï½–' in prev_line or lines_since_pending <= 2:
                        results.append(SurgeryCode(
                            code=pending_row['code'],
                            points=points,
                            name=pending_row['name'],
                            group=pending_row['group'],
                            subgroup=pending_row['subgroup'],
                            page=pending_row['page']
                        ))
                        pending_row = None
                    else:
                        # å¯èƒ½æ˜¯é»æ•¸åœ¨ code ä¹‹å‰çš„æƒ…æ³
                        last_points_candidate = points
                        lines_since_points = 0
                else:
                    last_points_candidate = points
                    lines_since_points = 0

            # æ›´æ–°è¨ˆæ•¸å™¨
            if pending_row:
                lines_since_pending += 1
            if last_points_candidate:
                lines_since_points += 1

            prev_line = line

    return results


# =============================================================================
# Deduplication & Normalization
# =============================================================================

def deduplicate_codes(codes: List[SurgeryCode]) -> List[SurgeryCode]:
    """
    å»é‡ï¼šåŒä¸€ code ä¿ç•™åç¨±æœ€é•·çš„ä¸€ç­†
    æª¢æŸ¥é»æ•¸ä¸€è‡´æ€§
    """
    by_code: Dict[str, List[SurgeryCode]] = {}

    for c in codes:
        if c.code not in by_code:
            by_code[c.code] = []
        by_code[c.code].append(c)

    results = []
    inconsistent = []

    for code, entries in by_code.items():
        # æª¢æŸ¥é»æ•¸ä¸€è‡´æ€§
        points_set = set(e.points for e in entries)
        if len(points_set) > 1:
            inconsistent.append((code, points_set))

        # å–åç¨±æœ€é•·çš„
        best = max(entries, key=lambda e: len(e.name))
        results.append(best)

    if inconsistent:
        print(f"âš ï¸ ç™¼ç¾ {len(inconsistent)} å€‹ code æœ‰ä¸ä¸€è‡´çš„é»æ•¸:")
        for code, pts in inconsistent[:5]:
            print(f"   {code}: {pts}")
        if len(inconsistent) > 5:
            print(f"   ... é‚„æœ‰ {len(inconsistent) - 5} å€‹")

    return sorted(results, key=lambda c: c.code)


# =============================================================================
# Output Writers
# =============================================================================

def write_csv(codes: List[SurgeryCode], path: Path):
    with open(path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['code', 'points', 'name', 'group', 'subgroup'])
        writer.writeheader()
        for c in codes:
            writer.writerow({
                'code': c.code,
                'points': c.points,
                'name': c.name,
                'group': c.group,
                'subgroup': c.subgroup
            })


def write_json(codes: List[SurgeryCode], path: Path):
    data = [asdict(c) for c in codes]
    # ç§»é™¤ page æ¬„ä½
    for d in data:
        d.pop('page', None)

    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def compute_hash(path: Path) -> str:
    with open(path, 'rb') as f:
        return hashlib.sha256(f.read()).hexdigest()[:16]


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='NHI å¥ä¿æ‰‹è¡“ç¢¼èƒå–å™¨ (ç¬¬ä¸ƒç¯€)')
    parser.add_argument('--pdf', required=True, help='form4-001.pdf è·¯å¾‘')
    parser.add_argument('--out', default='./data/packs/nhi_sec7', help='è¼¸å‡ºç›®éŒ„')
    args = parser.parse_args()

    pdf_path = Path(args.pdf)
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    if not pdf_path.exists():
        print(f"âŒ PDF æª”æ¡ˆä¸å­˜åœ¨: {pdf_path}")
        return 1

    print(f"ğŸ“„ é–‹å•Ÿ PDF: {pdf_path}")
    doc = fitz.open(pdf_path)
    print(f"   ç¸½é æ•¸: {len(doc)}")

    # æ‰¾ç¬¬ä¸ƒç¯€ç¯„åœ
    print("\nğŸ” å®šä½ç¬¬ä¸ƒç¯€ç¯„åœ...")
    start_page, end_page = find_section7_pages(doc)
    print(f"   ç¬¬ä¸ƒç¯€: ç¬¬ {start_page + 1} é  ~ ç¬¬ {end_page + 1} é  ({end_page - start_page + 1} é )")

    # èƒå–
    print("\nğŸ“Š èƒå–è¡“å¼ç¢¼...")
    raw_codes = extract_codes_from_pages(doc, start_page, end_page)
    print(f"   åŸå§‹ç­†æ•¸: {len(raw_codes)}")

    # å»é‡
    print("\nğŸ”„ å»é‡èˆ‡æ­£è¦åŒ–...")
    unique_codes = deduplicate_codes(raw_codes)
    print(f"   å”¯ä¸€ code: {len(unique_codes)}")

    # é©—æ”¶æ¸¬è©¦
    print("\nâœ… é©—æ”¶æ¸¬è©¦...")
    all_valid = all(CODE_ONLY.match(c.code) for c in unique_codes)
    print(f"   code æ ¼å¼æ­£ç¢º: {'âœ“' if all_valid else 'âœ—'}")
    print(f"   æ•¸é‡ > 1000: {'âœ“' if len(unique_codes) > 1000 else 'âœ—'} ({len(unique_codes)})")

    # è¼¸å‡º
    print("\nğŸ’¾ å¯«å…¥æª”æ¡ˆ...")
    csv_path = out_dir / 'nhi_surgery_codes_sec7.csv'
    json_path = out_dir / 'nhi_surgery_codes_sec7.json'

    write_csv(unique_codes, csv_path)
    write_json(unique_codes, json_path)

    csv_hash = compute_hash(csv_path)
    json_hash = compute_hash(json_path)

    print(f"   {csv_path} (SHA256: {csv_hash})")
    print(f"   {json_path} (SHA256: {json_hash})")

    # Summary
    print("\n" + "="*50)
    print("ğŸ“‹ Summary")
    print("="*50)
    print(f"   PDF: {pdf_path.name}")
    print(f"   Pages: {start_page + 1} - {end_page + 1}")
    print(f"   Unique codes: {len(unique_codes)}")
    print(f"   Groups: {len(set(c.group for c in unique_codes if c.group))}")

    # é»æ•¸çµ±è¨ˆ
    points_list = [c.points for c in unique_codes]
    print(f"   Points range: {min(points_list)} - {max(points_list)}")

    doc.close()
    return 0


if __name__ == '__main__':
    exit(main())
